{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import trange, tqdm\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "import xgboost as xgb\n",
    "from gensim import utils\n",
    "import gensim.parsing.preprocessing as gsp\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "# from imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/iebi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# embeddings_dict = {}\n",
    "# with open('./data/glove.6B.300d.txt', 'r', encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         vector = np.asarray(values[1:], \"float32\")\n",
    "#         embeddings_dict[word] = vector\n",
    "# print('Found %s word vectors.' % len(embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train = pd.read_json('./data/train_gold.json', lines=True, encoding=\"utf8\")\n",
    "test = pd.read_json('./data/dev_unlabeled.json', lines=True, encoding=\"utf8\")\n",
    "categories = pd.read_json('./data/categories.json', lines=True, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23366\n",
       "2     5482\n",
       "3     1678\n",
       "4      827\n",
       "5      389\n",
       "6      258\n",
       "Name: categories, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_categories = train.categories.apply(lambda x: len(x))\n",
    "num_categories.value_counts() # 這邊我可能會想只取第一類來作，剩下的就亂猜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(num_categories.value_counts().keys(), num_categories.value_counts().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只留第一個 (recall rate下降)\n",
    "# train[\"categories\"] = train[\"categories\"].apply(lambda x: [x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = [\n",
    "           gsp.strip_tags, \n",
    "           gsp.strip_punctuation,\n",
    "           gsp.strip_multiple_whitespaces,\n",
    "           gsp.strip_numeric,\n",
    "           gsp.remove_stopwords, \n",
    "           gsp.strip_short, \n",
    "           gsp.stem_text #  lower+ porter-stemmer\n",
    "          ]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = utils.to_unicode(text)\n",
    "    for f in filters:\n",
    "        text = f(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataCleansing(df):\n",
    "    df.text = df.text.apply(clean_text)\n",
    "    df.reply = df.reply.apply(clean_text)\n",
    "    df[\"total_text\"] = df.text +' '+ df.reply\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.06 s, sys: 0 ns, total: 3.06 s\n",
      "Wall time: 3.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = dataCleansing(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 388 ms, sys: 0 ns, total: 388 ms\n",
      "Wall time: 387 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = dataCleansing(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_categories = train.categories.apply(lambda x: len(x))\n",
    "# num_categories.value_counts() # 這邊我可能會想只取第一類來作，剩下的就亂猜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合併text & reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"token\"] = train.total_text.apply(lambda x: tknzr.tokenize(x))\n",
    "test[\"token\"] = test.total_text.apply(lambda x: tknzr.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "      <th>categories</th>\n",
       "      <th>mp4</th>\n",
       "      <th>total_text</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>agre song niall horan</td>\n",
       "      <td>oui oui</td>\n",
       "      <td>[yes]</td>\n",
       "      <td>6dc39e96b11275f064fdaed88273b45e.mp4</td>\n",
       "      <td>agre song niall horan oui oui</td>\n",
       "      <td>[agre, song, niall, horan, oui, oui]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>instal scottyfrommarket new track app answer f...</td>\n",
       "      <td></td>\n",
       "      <td>[no]</td>\n",
       "      <td>cfff051f05d8d3b7136c7d58ea6ad55f.mp4</td>\n",
       "      <td>instal scottyfrommarket new track app answer f...</td>\n",
       "      <td>[instal, scottyfrommarket, new, track, app, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>grow mum nigga despit black tell stop racist s...</td>\n",
       "      <td>join pour hot grit</td>\n",
       "      <td>[smh]</td>\n",
       "      <td>bf39e7bd9ad24354ce3ba6822b0104af.mp4</td>\n",
       "      <td>grow mum nigga despit black tell stop racist s...</td>\n",
       "      <td>[grow, mum, nigga, despit, black, tell, stop, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>rest head chest world feel heavi</td>\n",
       "      <td>😂😂😂😂😂</td>\n",
       "      <td>[wink]</td>\n",
       "      <td>173a707a04c277354a2f23cf01d6151e.mp4</td>\n",
       "      <td>rest head chest world feel heavi 😂😂😂😂😂</td>\n",
       "      <td>[rest, head, chest, world, feel, heavi, 😂, 😂, 😂]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>imagin hernandez will pull stunt screen block ...</td>\n",
       "      <td></td>\n",
       "      <td>[yes]</td>\n",
       "      <td>aab6d6bfb0c1382269ddba9b71cc8b7a.mp4</td>\n",
       "      <td>imagin hernandez will pull stunt screen block ...</td>\n",
       "      <td>[imagin, hernandez, will, pull, stunt, screen,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>31995</td>\n",
       "      <td>tell duck pull truck hook</td>\n",
       "      <td></td>\n",
       "      <td>[ok]</td>\n",
       "      <td>da53a69a7457bc3076483e9b2f8a5cbc.mp4</td>\n",
       "      <td>tell duck pull truck hook</td>\n",
       "      <td>[tell, duck, pull, truck, hook]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>31996</td>\n",
       "      <td>like exactli week awai cowboi make round pick ...</td>\n",
       "      <td></td>\n",
       "      <td>[scared]</td>\n",
       "      <td>ea5282d5016838a56a542f3179122715.mp4</td>\n",
       "      <td>like exactli week awai cowboi make round pick ...</td>\n",
       "      <td>[like, exactli, week, awai, cowboi, make, roun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>31997</td>\n",
       "      <td>absolut freak night mom’ cancer unemploy monei...</td>\n",
       "      <td></td>\n",
       "      <td>[hug]</td>\n",
       "      <td>14c6869cc1efc58deeeab656fbfcef6d.mp4</td>\n",
       "      <td>absolut freak night mom’ cancer unemploy monei...</td>\n",
       "      <td>[absolut, freak, night, mom, ’, cancer, unempl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>31998</td>\n",
       "      <td>receipt life memori weather instil famili coun...</td>\n",
       "      <td></td>\n",
       "      <td>[hearts]</td>\n",
       "      <td>1a4ad489e6398e7398278709c14e1b67.mp4</td>\n",
       "      <td>receipt life memori weather instil famili coun...</td>\n",
       "      <td>[receipt, life, memori, weather, instil, famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>31999</td>\n",
       "      <td>crazi “nanci pelosi weak person poor leader re...</td>\n",
       "      <td></td>\n",
       "      <td>[slow_clap]</td>\n",
       "      <td>4cbdfe097db25f630cb4b302f8a6429c.mp4</td>\n",
       "      <td>crazi “nanci pelosi weak person poor leader re...</td>\n",
       "      <td>[crazi, “, nanci, pelosi, weak, person, poor, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         idx                                               text  \\\n",
       "0          0                              agre song niall horan   \n",
       "1          1  instal scottyfrommarket new track app answer f...   \n",
       "2          2  grow mum nigga despit black tell stop racist s...   \n",
       "3          3                   rest head chest world feel heavi   \n",
       "4          4  imagin hernandez will pull stunt screen block ...   \n",
       "...      ...                                                ...   \n",
       "31995  31995                          tell duck pull truck hook   \n",
       "31996  31996  like exactli week awai cowboi make round pick ...   \n",
       "31997  31997  absolut freak night mom’ cancer unemploy monei...   \n",
       "31998  31998  receipt life memori weather instil famili coun...   \n",
       "31999  31999  crazi “nanci pelosi weak person poor leader re...   \n",
       "\n",
       "                    reply   categories                                   mp4  \\\n",
       "0                 oui oui        [yes]  6dc39e96b11275f064fdaed88273b45e.mp4   \n",
       "1                                 [no]  cfff051f05d8d3b7136c7d58ea6ad55f.mp4   \n",
       "2      join pour hot grit        [smh]  bf39e7bd9ad24354ce3ba6822b0104af.mp4   \n",
       "3                   😂😂😂😂😂       [wink]  173a707a04c277354a2f23cf01d6151e.mp4   \n",
       "4                                [yes]  aab6d6bfb0c1382269ddba9b71cc8b7a.mp4   \n",
       "...                   ...          ...                                   ...   \n",
       "31995                             [ok]  da53a69a7457bc3076483e9b2f8a5cbc.mp4   \n",
       "31996                         [scared]  ea5282d5016838a56a542f3179122715.mp4   \n",
       "31997                            [hug]  14c6869cc1efc58deeeab656fbfcef6d.mp4   \n",
       "31998                         [hearts]  1a4ad489e6398e7398278709c14e1b67.mp4   \n",
       "31999                      [slow_clap]  4cbdfe097db25f630cb4b302f8a6429c.mp4   \n",
       "\n",
       "                                              total_text  \\\n",
       "0                          agre song niall horan oui oui   \n",
       "1      instal scottyfrommarket new track app answer f...   \n",
       "2      grow mum nigga despit black tell stop racist s...   \n",
       "3                 rest head chest world feel heavi 😂😂😂😂😂   \n",
       "4      imagin hernandez will pull stunt screen block ...   \n",
       "...                                                  ...   \n",
       "31995                         tell duck pull truck hook    \n",
       "31996  like exactli week awai cowboi make round pick ...   \n",
       "31997  absolut freak night mom’ cancer unemploy monei...   \n",
       "31998  receipt life memori weather instil famili coun...   \n",
       "31999  crazi “nanci pelosi weak person poor leader re...   \n",
       "\n",
       "                                                   token  \n",
       "0                   [agre, song, niall, horan, oui, oui]  \n",
       "1      [instal, scottyfrommarket, new, track, app, an...  \n",
       "2      [grow, mum, nigga, despit, black, tell, stop, ...  \n",
       "3       [rest, head, chest, world, feel, heavi, 😂, 😂, 😂]  \n",
       "4      [imagin, hernandez, will, pull, stunt, screen,...  \n",
       "...                                                  ...  \n",
       "31995                    [tell, duck, pull, truck, hook]  \n",
       "31996  [like, exactli, week, awai, cowboi, make, roun...  \n",
       "31997  [absolut, freak, night, mom, ’, cancer, unempl...  \n",
       "31998  [receipt, life, memori, weather, instil, famil...  \n",
       "31999  [crazi, “, nanci, pelosi, weak, person, poor, ...  \n",
       "\n",
       "[32000 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_token = pd.concat([train.token, test.token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 34659\n",
      "CPU times: user 27.1 s, sys: 207 ms, total: 27.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(vocab_token, size=300, window=5, min_count=1, iter=20, workers=8)\n",
    "words = model.wv.vocab.keys()\n",
    "vocab_size = len(words)\n",
    "print(\"Vocab size\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer() # using default parameters\n",
    "# vectorizer.fit(train['total_text'])\n",
    "# train_X = vectorizer.transform(train['total_text'])\n",
    "# test_X = vectorizer.transform(test['total_text'])\n",
    "\n",
    "# train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentenceToVec(df):\n",
    "    data = []\n",
    "    for sentence in df.token:\n",
    "        sentence_vector = np.zeros(300)\n",
    "        count = 0\n",
    "\n",
    "        for word in sentence:\n",
    "            # Glove\n",
    "    #         embedding_vector = embeddings_dict.get(word)\n",
    "            # Word2Vec\n",
    "            try:\n",
    "                sentence_vector += np.array(model.wv.get_vector(word))\n",
    "                count += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        if count == 0:\n",
    "            data.append(sentence_vector)\n",
    "        else:\n",
    "            data.append(sentence_vector / count)\n",
    "\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for i, col in enumerate(categories.iloc[0]):\n",
    "    label_dict[col] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_onehot(labels):\n",
    "    onehot = np.zeros(len(label_dict))\n",
    "    for l in labels:\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def label_to_all(datasets):\n",
    "    label = []\n",
    "    for row in datasets[\"categories\"]:\n",
    "        label.append(label_to_onehot(row))\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sentenceToVec(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_to_all(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 300)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 43)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### 將訓練資料分為訓練組及驗證組\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_X, labels, test_size=0.1, random_state=42)\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = list(label_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sentenceToVec(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf09a0342854daf9cdf14cfc019b8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=43.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iebi/anaconda3/envs/pytorch/lib/python3.6/site-packages/xgboost/core.py:614: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase memory consumption\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 4min 54s, sys: 4.33 s, total: 4min 58s\n",
      "Wall time: 59.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Predict_results= []\n",
    "for i in tqdm(range(43)):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train.T[i])\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    params = {\n",
    "    'booster': 'gbtree',\n",
    "    'learning_rate':0.1,\n",
    "    'n_etimators': 300, # 300\n",
    "    'max_depth': 10, # 12, 3isbad\n",
    "    'min_child_weight':5,\n",
    "    'gamma':0.5,\n",
    "    'colsample_bytree':0.8,\n",
    "    'subsample':0.8,\n",
    "    'reg_alpha':1,\n",
    "    'reg_lambda':0.1,\n",
    "    'objective':'binary:logistic',\n",
    "    'nthread': 8,                 \n",
    "    }\n",
    "    bst = xgb.train(params, dtrain)\n",
    "    pred = bst.predict(dtest)\n",
    "    Predict_results.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_results = np.array(Predict_results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_res = []\n",
    "for row in Predict_results:\n",
    "    tmp = np.zeros(43)\n",
    "    tmp[row.argsort()[-6:]] = 1\n",
    "    Predict_res.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_res = np.array(Predict_res).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(y_pred, y_test):\n",
    "    rate = []\n",
    "    for i, row in enumerate(y_test):\n",
    "        rating = 0\n",
    "        for j, ans in enumerate(row):\n",
    "            if ans == 1 and y_pred[i][j] == 1:\n",
    "                rating += 1\n",
    "        rate.append(rating/y_test[i].sum())\n",
    "        acc = np.array(rate).sum() / len(y_pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation(Predict_res, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('./data/dev_unlabeled.json', lines=True, encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 43)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32000</td>\n",
       "      <td>Drop your cash app, use hashtag #BailoutHumansNow</td>\n",
       "      <td>$tyratomaro #BailoutHumans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32001</td>\n",
       "      <td>After interviewing with a few incredible peopl...</td>\n",
       "      <td>CONGRATS!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32002</td>\n",
       "      <td>I know GTC festival not happening next month b...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32003</td>\n",
       "      <td>Lordy, my daughter just said, “I wonder how th...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32004</td>\n",
       "      <td>THE UNEMPLOYMENT CLAIM SYSTEM SUCKS SO MUCH DICK</td>\n",
       "      <td>Watching everyone else get their weekly unempl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>35995</td>\n",
       "      <td>WHY TF DO LOCALS LIKE USING GIFS SO MUCH THEYR...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>35996</td>\n",
       "      <td>Choose a gif that best describes the Republica...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>35997</td>\n",
       "      <td>fuck soon</td>\n",
       "      <td>When soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>35998</td>\n",
       "      <td>SOMEBODY GIVE ME A HUG PLEASE \\n\\nI would real...</td>\n",
       "      <td>I hope you’re doing okay or will be okay!! Sen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>35999</td>\n",
       "      <td>Alright, I gotta give my daughter a bath. \\n\\n...</td>\n",
       "      <td>pick meeee! haha $astich1997 but on a serious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                                               text  \\\n",
       "0     32000  Drop your cash app, use hashtag #BailoutHumansNow   \n",
       "1     32001  After interviewing with a few incredible peopl...   \n",
       "2     32002  I know GTC festival not happening next month b...   \n",
       "3     32003  Lordy, my daughter just said, “I wonder how th...   \n",
       "4     32004   THE UNEMPLOYMENT CLAIM SYSTEM SUCKS SO MUCH DICK   \n",
       "...     ...                                                ...   \n",
       "3995  35995  WHY TF DO LOCALS LIKE USING GIFS SO MUCH THEYR...   \n",
       "3996  35996  Choose a gif that best describes the Republica...   \n",
       "3997  35997                                          fuck soon   \n",
       "3998  35998  SOMEBODY GIVE ME A HUG PLEASE \\n\\nI would real...   \n",
       "3999  35999  Alright, I gotta give my daughter a bath. \\n\\n...   \n",
       "\n",
       "                                                  reply  \n",
       "0                            $tyratomaro #BailoutHumans  \n",
       "1                                         CONGRATS!!!!!  \n",
       "2                                                        \n",
       "3                                                        \n",
       "4     Watching everyone else get their weekly unempl...  \n",
       "...                                                 ...  \n",
       "3995                                                     \n",
       "3996                                                     \n",
       "3997                                          When soon  \n",
       "3998  I hope you’re doing okay or will be okay!! Sen...  \n",
       "3999  pick meeee! haha $astich1997 but on a serious ...  \n",
       "\n",
       "[4000 rows x 3 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_json(df, results):\n",
    "    cat_results = []\n",
    "    for row in results:\n",
    "        tmp = []\n",
    "        for i, res in enumerate(row):\n",
    "            if res == 1:\n",
    "                tmp.append(CATEGORIES[i])\n",
    "        cat_results.append(tmp)\n",
    "    df[\"categories\"] = cat_results\n",
    "    df = df[['idx', 'categories', 'reply', 'text']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = submit_json(test, Predict_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_json('dev.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>categories</th>\n",
       "      <th>reply</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32000</td>\n",
       "      <td>[applause, dance, good_luck, happy_dance, plea...</td>\n",
       "      <td>$tyratomaro #BailoutHumans</td>\n",
       "      <td>Drop your cash app, use hashtag #BailoutHumansNow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32001</td>\n",
       "      <td>[agree, applause, happy_dance, sigh, slow_clap...</td>\n",
       "      <td>CONGRATS!!!!!</td>\n",
       "      <td>After interviewing with a few incredible peopl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32002</td>\n",
       "      <td>[applause, facepalm, no, omg, smh, yes]</td>\n",
       "      <td></td>\n",
       "      <td>I know GTC festival not happening next month b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32003</td>\n",
       "      <td>[agree, applause, facepalm, oh_snap, omg, yes]</td>\n",
       "      <td></td>\n",
       "      <td>Lordy, my daughter just said, “I wonder how th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32004</td>\n",
       "      <td>[agree, applause, facepalm, idk, seriously, yes]</td>\n",
       "      <td>Watching everyone else get their weekly unempl...</td>\n",
       "      <td>THE UNEMPLOYMENT CLAIM SYSTEM SUCKS SO MUCH DICK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>35995</td>\n",
       "      <td>[agree, applause, facepalm, sigh, slow_clap, yes]</td>\n",
       "      <td></td>\n",
       "      <td>WHY TF DO LOCALS LIKE USING GIFS SO MUCH THEYR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>35996</td>\n",
       "      <td>[eww, facepalm, scared, seriously, smh, thumbs...</td>\n",
       "      <td></td>\n",
       "      <td>Choose a gif that best describes the Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>35997</td>\n",
       "      <td>[agree, applause, hug, shocked, yes, you_got_t...</td>\n",
       "      <td>When soon</td>\n",
       "      <td>fuck soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>35998</td>\n",
       "      <td>[agree, awww, hearts, hug, please, you_got_this]</td>\n",
       "      <td>I hope you’re doing okay or will be okay!! Sen...</td>\n",
       "      <td>SOMEBODY GIVE ME A HUG PLEASE \\n\\nI would real...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>35999</td>\n",
       "      <td>[applause, good_luck, happy_dance, hug, sigh, ...</td>\n",
       "      <td>pick meeee! haha $astich1997 but on a serious ...</td>\n",
       "      <td>Alright, I gotta give my daughter a bath. \\n\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idx                                         categories  \\\n",
       "0     32000  [applause, dance, good_luck, happy_dance, plea...   \n",
       "1     32001  [agree, applause, happy_dance, sigh, slow_clap...   \n",
       "2     32002            [applause, facepalm, no, omg, smh, yes]   \n",
       "3     32003     [agree, applause, facepalm, oh_snap, omg, yes]   \n",
       "4     32004   [agree, applause, facepalm, idk, seriously, yes]   \n",
       "...     ...                                                ...   \n",
       "3995  35995  [agree, applause, facepalm, sigh, slow_clap, yes]   \n",
       "3996  35996  [eww, facepalm, scared, seriously, smh, thumbs...   \n",
       "3997  35997  [agree, applause, hug, shocked, yes, you_got_t...   \n",
       "3998  35998   [agree, awww, hearts, hug, please, you_got_this]   \n",
       "3999  35999  [applause, good_luck, happy_dance, hug, sigh, ...   \n",
       "\n",
       "                                                  reply  \\\n",
       "0                            $tyratomaro #BailoutHumans   \n",
       "1                                         CONGRATS!!!!!   \n",
       "2                                                         \n",
       "3                                                         \n",
       "4     Watching everyone else get their weekly unempl...   \n",
       "...                                                 ...   \n",
       "3995                                                      \n",
       "3996                                                      \n",
       "3997                                          When soon   \n",
       "3998  I hope you’re doing okay or will be okay!! Sen...   \n",
       "3999  pick meeee! haha $astich1997 but on a serious ...   \n",
       "\n",
       "                                                   text  \n",
       "0     Drop your cash app, use hashtag #BailoutHumansNow  \n",
       "1     After interviewing with a few incredible peopl...  \n",
       "2     I know GTC festival not happening next month b...  \n",
       "3     Lordy, my daughter just said, “I wonder how th...  \n",
       "4      THE UNEMPLOYMENT CLAIM SYSTEM SUCKS SO MUCH DICK  \n",
       "...                                                 ...  \n",
       "3995  WHY TF DO LOCALS LIKE USING GIFS SO MUCH THEYR...  \n",
       "3996  Choose a gif that best describes the Republica...  \n",
       "3997                                          fuck soon  \n",
       "3998  SOMEBODY GIVE ME A HUG PLEASE \\n\\nI would real...  \n",
       "3999  Alright, I gotta give my daughter a bath. \\n\\n...  \n",
       "\n",
       "[4000 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
